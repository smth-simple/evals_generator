{"sections": [{"title": "1. Executive Summary", "content": "## Executive Summary\n\n**The Test Model wins decisively, but the victory is behavioral\u2014not cognitive.**\n\n[[FIGURE: viz_00_summary.png]]\n\nModel B (Test Model) achieves a **+11.6 percentage point net win rate advantage** over Model A (Base Model), with 38.6% of evaluations favoring the Test Model versus 27.0% for the Base Model. This represents a clear and commercially significant improvement in user preference across the global evaluation corpus of 18,726 responses.\n\n### Primary Performance Drivers\n\nThe Test Model's advantage stems from **systematic improvements in response formatting and conciseness** rather than enhanced factual accuracy or reasoning capability. Analysis of the Likert distribution by region (see viz_17) reveals that wins concentrate in categories where behavioral refinement matters most: Brainstorming (+21.0pp), Creative Writing (+16.1pp), and Chatbot interactions (+17.2pp). Conversely, the Test Model shows **no improvement or slight regression** in reference-grounded tasks requiring factual precision\u2014Closed Q&A actually favors the Base Model by 1.9 percentage points.\n\nThe response length tendency analysis (see viz_18) quantifies this behavioral shift: the Test Model reduced \"very verbose\" responses from 13.9% to 7.1% globally, while \"neutral\" length responses increased from 50.5% to 64.1%. This 13.6 percentage point shift toward appropriate response length correlates directly with the composite quality improvements observed across languages.\n\n### Critical Risk Areas\n\nDespite aggregate gains, the Test Model exhibits **meaningful regressions** in three domains requiring immediate attention:\n\n**1. Arabic Dialect Differentiation Failure:** ar_AE shows only +1.0pp net win rate\u2014the lowest of any language\u2014with localization major error rates actually increasing (+0.8pp). Qualitative analysis confirms persistent defaulting to Modern Standard Arabic regardless of dialect-specific prompts.\n\n**2. Eastern European Language Modeling Collapse:** ru_RU and uk_UA maintain catastrophic error rates exceeding 90% for both models, with the Test Model achieving only marginal improvement (-2.4pp and -2.9pp respectively). This reflects fundamental pre-training deficiencies rather than fine-tuning gaps.\n\n**3. Factual Accuracy Stagnation:** Truthfulness major error rates improved only -4.9pp globally, with hallucination rates remaining essentially flat in high-stakes domains. The Test Model wins user preference without demonstrably improving correctness.\n\n### Strategic Implication\n\nThe Test Model represents a **better-behaved assistant**, not a smarter one. This profile is commercially valuable\u2014users demonstrably prefer concise, well-formatted responses\u2014but creates risk exposure in applications requiring factual reliability. Deployment should prioritize use cases where stylistic quality drives value while implementing guardrails for reference-critical domains."}, {"title": "2. Model Comparison Overview", "content": "## 2. Model Performance Deep Dive\n\n### 2.1 Overall Comparison (Quality Scores)\n\nThe composite quality analysis reveals the full spectrum of Test Model improvements across all evaluation dimensions, with substantial variation by language market.\n\n[[FIGURE: viz_16_composite_score.png]]\n\nThe Total Quality Delta metric aggregates improvements across Instruction Following (IF), Truthfulness (TF), Localization (LOC), Style, Harm avoidance, and Response Length/Redundancy (RL). Three distinct performance tiers emerge from this analysis:\n\n**Top Performers (Total Delta > 6.0):**\n- hi_Latn (Romanized Hindi): +6.99 total delta, driven primarily by RL (+2.91) and TF (+1.53) improvements\n- pl_PL (Polish): +6.65 total delta, with the highest Style improvement (+1.56) of any language\n- ja_JP (Japanese): +6.58 total delta, balanced gains across IF (+1.00), TF (+1.00), and RL (+2.54)\n- id_ID (Indonesian): +6.47 total delta, strongest IF improvement (+1.46) globally\n- fr_CA (French Canadian): +6.15 total delta, highest RL improvement (+2.64) of any language\n- no_NO (Norwegian): +6.15 total delta, strongest IF improvement among Nordic languages (+1.62)\n\n**Moderate Performers (Total Delta 3.0\u20136.0):**\nMost Western European and East Asian markets fall into this range, showing consistent but unremarkable improvements. German (de_DE) at +2.77 and French (fr_FR) at +1.57 represent the lower bounds of acceptable improvement.\n\n**Underperformers (Total Delta < 2.0):**\n- ar_AE (UAE Arabic): +1.74 total delta\u2014the lowest of any language\n- ko_KR (Korean): +1.90 total delta\n- zh_HK (Hong Kong Chinese): +1.87 total delta\n- en_US (US English): +0.92 total delta\u2014notably, the primary development language shows minimal gains\n\nThe RL (Response Length/Redundancy) dimension contributes the largest share of improvement across nearly all languages, averaging +1.28 points globally. This confirms that verbosity reduction represents the Test Model's primary behavioral shift. However, this improvement is not uniform: Russian shows +2.49 RL improvement despite catastrophic overall error rates, suggesting the model learned to be concise without learning to be correct.\n\nMinor error analysis provides additional diagnostic granularity on where refinements occurred versus where fundamental issues persist.\n\n[[FIGURE: viz_15_minor_error_deltas.png]]\n\nThe minor error deltas reveal a critical pattern: **Style improvements are inconsistent while RL improvements are universal**. Key observations:\n\n- **Universal RL improvement:** 33 of 36 languages show negative RL_Minor_delta (fewer redundancy issues), with only en_US (+2.49), ru_RU (+6.39), and uk_UA (+8.99) showing increases. The Cyrillic languages' positive RL deltas likely reflect evaluator difficulty distinguishing redundancy from incoherence in gibberish outputs.\n\n- **Style regression in key markets:** en_IN shows -0.60 Style delta despite strong overall performance, while zh_TW shows -8.74 Style_Minor_delta\u2014the worst style regression of any language. This suggests the Test Model's conciseness sometimes manifests as stylistic awkwardness.\n\n- **Localization improvements concentrated in specific markets:** he_IL shows +11.26 LOC_Minor_delta improvement, while es_ES shows -10.45 LOC_Minor_delta (worsening). Spanish localization requires investigation.\n\n### 2.2 Prompt Categories\n\nTask type analysis reveals the Test Model's strengths cluster around open-ended generation while reference-based tasks show minimal or negative improvement.\n\n[[FIGURE: viz_13_prompt_cat_comparison.png]]\n\n**Generative Task Dominance:**\nThe Test Model's largest win rate advantages occur in tasks where stylistic polish matters most:\n- Brainstorming: +21.0pp net win rate (21.6% \u2192 42.6%), with error rate dropping from 68.1% to 56.9%\n- Chatbot: +17.2pp net win rate, error rate reduction of 6.8pp\n- Open Q&A: +17.1pp net win rate, largest absolute error reduction (-7.4pp)\n- Creative Writing: +16.1pp net win rate, error rate drops from 64.2% to 55.8%\n\nThese categories share a common characteristic: success depends more on coherence, tone, and structure than on factual precision. The Test Model's behavioral refinements\u2014reduced verbosity, better formatting, improved instruction compliance\u2014translate directly into user preference without requiring improved knowledge.\n\n**Reference-Based Task Stagnation:**\nTasks requiring factual grounding show minimal or negative improvement:\n- Closed Q&A: -1.9pp net win rate (37.8% \u2192 35.9%), with **identical error rates** (30.9% for both models)\n- Extraction: -0.1pp net win rate, error rate drops only 1.8pp\n- Summarization: +5.8pp net win rate, modest 2.1pp error reduction\n\nThe Closed Q&A regression is particularly significant: this is the only category where the Base Model outperforms the Test Model on win rate. The unchanged 30.9% error rate indicates that whatever improvements the Test Model achieved in style and formatting, it did not improve its ability to answer factual questions correctly.\n\n**The Classification Anomaly:**\nClassification tasks show only +4.1pp net win rate despite a 3.4pp error reduction. This suggests users perceive classification accuracy as table stakes rather than a differentiator\u2014correct classifications don't generate enthusiasm, but incorrect ones generate dissatisfaction.\n\n### 2.3 Failure Patterns\n\nTruthfulness subcategory analysis by region exposes the specific failure modes driving persistent error rates.\n\n[[FIGURE: viz_09_tf_subcat_by_model_region.png]]\n\n**Hallucination Remains the Dominant Failure Mode:**\nAcross all regions, hallucination (fabricating non-existent facts, entities, or events) represents the largest TF subcategory. Regional hallucination rates for the Test Model:\n- Eastern Europe: 45.2% (down from 49.4% for Base)\u2014still catastrophic\n- MENA: 22.6% (down from 24.3%)\u2014minimal improvement\n- East Asia: 22.6% contextual inaccuracies dominate over pure hallucination\n- Western Europe: 15-18% range, more manageable\n\nThe Eastern European hallucination rate of 45.2% means nearly half of all responses contain fabricated information. Combined with the 48.9% major spelling/grammar rate, this indicates fundamental language modeling failures rather than fine-tuning deficiencies.\n\n**Factual Incorrectness Distribution:**\nFactually incorrect responses (stating verifiably false information) show more regional variation:\n- South Asia: 15.8% (Test) vs 22.8% (Base)\u2014meaningful 7pp improvement\n- Western Europe: 17.4% (Test) vs 21.0% (Base)\u2014consistent improvement\n- MENA: 19.1% (Test) vs 19.3% (Base)\u2014essentially unchanged\n\n**Cultural Misalignment Emergence:**\nMENA shows a concerning pattern: Cultural Misalignments increased from 2.2% (Base) to 3.6% (Test)\u2014a 63% relative increase. While small in absolute terms, this suggests the Test Model's training may have inadvertently reduced cultural sensitivity in pursuit of other optimizations.\n\n**The Dialect Problem Quantified:**\nWrong Dialect/Language errors in Arabic markets:\n- ar_AE: 38.1% (Test) vs 35.1% (Base)\u2014**worsening**\n- ar_SA: 41.0% (Test) vs 36.5% (Base)\u2014**worsening**\n- ar_EG: 24.9% (Test) vs 21.2% (Base)\u2014**worsening**\n\nThis is the only error category where the Test Model consistently underperforms the Base Model. The model has learned to default to Modern Standard Arabic regardless of dialect instructions, creating a ceiling on MENA market performance that behavioral refinements cannot overcome."}, {"title": "3. Language-Specific Analysis", "content": "## 3. Regional Deep Dive\n\nThe Test Model's +11.6 percentage point global advantage obscures substantial regional variation, with net win rates ranging from +23.5pp (Indonesian) to just +0.7pp (US English). This section examines the geographic patterns, systematic failure modes, and areas of evaluator disagreement that reveal where the model succeeds through genuine improvement versus where wins mask persistent quality issues.\n\n[[FIGURE: viz_00_language__overall_deltas__pref__mean_.png]]\n\n### 3.1 Regional Patterns\n\n**Top Performers: The Formatting Victory Cluster**\n\nThe highest-performing languages share a common signature: dramatic reductions in response length and redundancy issues rather than improvements in factual accuracy.\n\n| Language | Net Win Rate | RL Major Delta | TF Major Delta |\n|----------|--------------|----------------|----------------|\n| Indonesian (id_ID) | +23.5pp | -10.9pp | -7.6pp |\n| Polish (pl_PL) | +21.2pp | -11.5pp | -9.1pp |\n| Hebrew (he_IL) | +19.4pp | -10.7pp | -3.6pp |\n| Brazilian Portuguese (pt_BR) | +18.3pp | -7.3pp | -4.9pp |\n| French Canadian (fr_CA) | +18.1pp | -10.1pp | -9.7pp |\n\nThese five languages account for the Test Model's most impressive wins, yet the pattern is consistent: Response Length improvements exceed Truthfulness improvements by 3-7 percentage points. The Test Model wins by being more concise, not more accurate.\n\n**Underperformers: The Localization Ceiling**\n\nAt the opposite extreme, several major markets show minimal improvement despite the global trend:\n\n| Language | Net Win Rate | LOC Major Delta | Core Issue |\n|----------|--------------|-----------------|------------|\n| US English (en_US) | +0.7pp | -0.9pp | Baseline already strong |\n| UAE Arabic (ar_AE) | +1.0pp | +0.8pp (worse) | Dialect mismatch |\n| Simplified Chinese (zh_CN) | +4.0pp | -1.9pp | Cultural context gaps |\n| Korean (ko_KR) | +4.1pp | -0.4pp | Localization stagnation |\n| Saudi Arabic (ar_SA) | +5.8pp | +0.2pp (flat) | MSA default behavior |\n\nThe Arabic variants reveal a critical failure: the Test Model shows *worsening* localization in ar_AE (+0.8pp more major errors) and no improvement in ar_SA. Both languages suffer from the model's persistent default to Modern Standard Arabic regardless of dialect requests.\n\n**Regional Aggregate View**\n\nWhen grouped by region, Eastern Europe shows the highest net win rate (+17.2pp) but also maintains the highest error rates (81.3% for Test Model). This paradox\u2014winning while still failing\u2014reflects user preference for marginally better output even when both models produce unacceptable results.\n\n| Region | Net Win Rate | Test Error Rate | Quality Paradox |\n|--------|--------------|-----------------|-----------------|\n| Eastern Europe | +17.2pp | 81.3% | Wins on relative improvement |\n| Southeast Asia | +14.4pp | 48.9% | Genuine quality gains |\n| South Asia | +5.5pp | 38.9% | Smallest improvement, lowest errors |\n\n### 3.2 Common Failures\n\n**Failure Mode #1: Dialect Blindness (MENA Region)**\n\nThe most systematic failure occurs in Arabic-speaking markets where the model consistently produces Modern Standard Arabic (MSA) regardless of explicit dialect requests. Across ar_AE, ar_EG, and ar_SA, \"Wrong Dialect/Language\" appears in the top five error subcategories for both models, with the Test Model showing no improvement or slight regression.\n\nIn UAE Arabic specifically, the Test Model produces dialect errors in 38.1% of responses compared to 35.1% for the Base Model\u2014a 3 percentage point worsening. Evaluator notes consistently document responses \"fully written in MSA instead\" when Emirati, Egyptian, or Saudi dialects were explicitly requested.\n\n**Failure Mode #2: Fundamental Language Modeling Collapse (Cyrillic Languages)**\n\nRussian and Ukrainian represent catastrophic failure cases where both models produce near-unintelligible output. The Test Model maintains 90%+ error rates in both languages despite showing the highest regional net win rate.\n\n| Language | Test Error Rate | Major Grammar Issues | Hallucination Rate |\n|----------|-----------------|---------------------|-------------------|\n| Russian (ru_RU) | 90.1% | 48.9% | 45.2% |\n| Ukrainian (uk_UA) | 90.0% | 69.3% | 58.4% |\n\nEvaluator documentation reveals \"nonsensical words\" and \"gibberish\" appearing at rates suggesting fundamental gaps in Cyrillic language modeling rather than fine-tuning deficiencies. The Test Model's +15.8pp win rate in Russian reflects preference for slightly less broken output, not acceptable quality.\n\n**Failure Mode #3: Hallucination Persistence**\n\nDespite improvements in formatting and conciseness, hallucination rates remain stubbornly high across languages. The global hallucination rate drops only marginally, with several languages showing no improvement:\n\n- Hebrew: 22.7% (Base) \u2192 22.3% (Test) = -0.4pp\n- Arabic UAE: 19.3% (Base) \u2192 19.1% (Test) = -0.2pp\n- Finnish: 22.8% (Base) \u2192 20.7% (Test) = -2.1pp\n\nThe Test Model has learned to be concise but has not learned to verify facts before stating them.\n\n### 3.3 High Disagreement Areas\n\n[[FIGURE: viz_20_likert_disparity_top_8.png]]\n\n**Polarized Evaluation Patterns**\n\nSeveral languages show bimodal Likert distributions indicating evaluator disagreement about model quality. Japanese (ja_JP) exemplifies this pattern with 30.2% of evaluations at Likert 5 (slight Test preference) but also 24.5% at Likert 3 (slight Base preference)\u2014the highest Likert 3 rate among top performers.\n\nThe Turkish (tr_TR) distribution reveals extreme polarization: only 3.9% of evaluations land at the neutral Likert 4, while 41.1% fall at Likert 5 and 30.6% at Likert 3. This suggests the Test Model either clearly succeeds or clearly fails with minimal middle ground.\n\n**Task-Type Disagreement**\n\nHigh disagreement correlates with task complexity. For \"Hard Prompts\" (multi-constraint or expert-level queries), the Test Model shows:\n- Win rate improvement: +10.8pp\n- Error rate reduction: only -4.6pp (64.2% \u2192 59.6%)\n\nThis gap between preference improvement and error reduction indicates evaluators reward stylistic improvements even when substantive errors persist. The model wins the presentation contest while still failing the accuracy test in approximately 60% of difficult queries.\n\n**The Quality-Preference Divergence**\n\nFrench Canadian (fr_CA) demonstrates the sharpest divergence between quality metrics and preference outcomes. It achieves the highest Total Quality Delta (+6.15) driven primarily by Response Length improvements (+2.64 contribution), yet its Truthfulness improvement (+1.49) ranks only mid-tier. Users strongly prefer the cleaner, more concise output even when factual accuracy improvements are modest.\n\nThis pattern\u2014preference outpacing accuracy\u2014defines the Test Model's regional performance story. The model has become a better editor without becoming a better researcher."}, {"title": "4. LMArena-Focused Analysis", "content": "## 4. LMArena Performance Analysis\n\nThe LMArena evaluation framework segments prompts by difficulty and complexity, revealing where the Test Model's improvements hold up under pressure\u2014and where they break down.\n\n[[FIGURE: viz_14_lmarena_category_comparison.png]]\n\n### 4.1 LMArena Takeaways\n\nThe Test Model maintains its advantage across all LMArena categories, but the magnitude of improvement varies significantly by prompt difficulty:\n\n| Category | Base Win % | Test Win % | Net Gain | Error Rate Reduction |\n|----------|------------|------------|----------|---------------------|\n| Hard Prompts | 26.9% | 37.7% | +10.8pp | -4.6pp |\n| Expert Prompts | 29.1% | 38.5% | +9.4pp | -4.5pp |\n| Complex Prompts (IF) | 27.4% | 37.8% | +10.3pp | -4.9pp |\n| Longer Queries | 32.8% | 37.7% | +4.9pp | -1.3pp |\n\nThe consistent ~10 percentage point win rate improvement across Hard, Expert, and Complex categories suggests the Test Model's behavioral refinements\u2014conciseness, formatting, instruction compliance\u2014translate effectively even when prompts demand more sophisticated reasoning. However, error rate reductions remain modest (4-5 percentage points), indicating that while the Test Model produces more preferred outputs, it hasn't fundamentally improved its ability to avoid mistakes on challenging tasks.\n\n### 4.2 Hard Prompts\n\nHard Prompts represent the most demanding subset, with 7,301 evaluations testing multi-step reasoning, domain expertise, and constraint handling. The Test Model achieves a 37.7% win rate against the Base Model's 26.9%, yet both models still fail more often than they succeed\u2014the Test Model's error rate of 59.6% means nearly three in five Hard Prompt responses contain major errors.\n\nThis pattern reveals a ceiling effect: the Test Model wins by being cleaner and more compliant, not by solving problems the Base Model couldn't. When a Hard Prompt requires factual precision\u2014such as discussing an author's bibliography while maintaining a specific persona\u2014the Test Model's stylistic improvements cannot compensate for underlying knowledge gaps. The 59.6% error rate on Hard Prompts should be compared to the 56.9% error rate on Brainstorming tasks; the Test Model struggles nearly as much with complex factual queries as it does with open-ended generation, despite the latter being a strength area.\n\n### 4.3 Longer Queries\n\nLonger Queries (1,989 evaluations) present a counterintuitive finding: despite the Test Model's demonstrated strength in verbosity control, it shows the smallest improvement in this category (+4.9pp net win rate versus +10.8pp on Hard Prompts). Error rate reduction is minimal at just 1.3 percentage points.\n\n[[FIGURE: viz_18_rl_tendency.png]]\n\nThe verbosity distribution data illuminates why. Globally, the Test Model shifted dramatically toward neutral response lengths\u2014\"Neutral (0)\" responses increased from 50.5% to 64.1% (+13.6pp), while \"Very Verbose (+2)\" dropped from 13.9% to 7.1%. This recalibration works well for standard prompts but may overcorrect on Longer Queries, where users explicitly request comprehensive responses.\n\nRegional variations reinforce this interpretation. In South Asia, neutral responses jumped from 55.9% to 76.0%, and in Latin America from 61.9% to 75.8%. These dramatic shifts toward conciseness drive wins in most contexts but create friction when prompts demand thoroughness. The Test Model has learned to be brief\u2014but not necessarily to calibrate length to user intent.\n\nThe implication is clear: the Test Model's length optimization is a heuristic, not a contextual judgment. It defaults to shorter responses regardless of whether the prompt warrants them, producing diminishing returns on queries where verbosity would be appropriate."}, {"title": "5. Recommendations", "content": "## Section 5: Release Decision, Training Roadmap & Prompt Engineering\n\n### 5.1 Release Decision Framework\n\n**Recommendation: CONDITIONAL RELEASE with Regional Gating**\n\nThe Test Model demonstrates a statistically significant +11.6 percentage point net win rate advantage over the Base Model, representing meaningful improvement in user preference across 27 of 36 language codes. However, the improvement profile reveals critical deployment considerations that necessitate a tiered release strategy rather than universal deployment.\n\n**Release Tier Structure:**\n\n| Tier | Markets | Rationale | Action |\n|------|---------|-----------|--------|\n| **Immediate Release** | pt_BR, fr_CA, ja_JP, id_ID, pl_PL, tr_TR, nl_NL, it_IT, sv_SE | Net win rates +14pp to +23pp with error rate reductions of 8-12pp | Deploy with standard monitoring |\n| **Monitored Release** | en_GB, en_AU, de_DE, es_ES, es_MX, no_NO, da_DK, fi_FI, th_TH, vi_VN, zh_TW, zh_HK | Net win rates +9pp to +15pp; improvements are behavioral rather than factual | Deploy with enhanced quality monitoring on factual queries |\n| **Delayed Release** | ar_AE, ar_SA, ar_EG, en_US, zh_CN, ko_KR | Net win rates below +6pp; dialect failures (MENA) or minimal differentiation (en_US) | Hold for targeted fine-tuning |\n| **Do Not Release** | ru_RU, uk_UA | 90%+ error rates persist; fundamental language modeling failures produce incoherent output | Requires pre-training intervention |\n\nThe quantitative justification for this tiering emerges from the composite quality delta scores. Markets in the immediate release tier show Total Quality Deltas ranging from +4.47 (it_IT) to +6.65 (pl_PL), indicating broad improvement across instruction following, truthfulness, localization, and response length dimensions. The delayed release tier shows Total Quality Deltas below +2.8, with ar_AE at just +1.74\u2014the lowest of any market.\n\n**Critical Risk Assessment:**\n\nThe Test Model's improvement profile creates a specific risk pattern: users will perceive higher quality due to formatting and conciseness improvements while receiving equivalent or slightly worse factual accuracy. In domains where truthfulness is paramount\u2014medical queries, financial advice, legal information\u2014this creates a satisfaction-accuracy gap that could generate downstream liability.\n\nThe Closed Q&A category shows a -1.9 percentage point regression in win rate, the only task category where the Test Model loses ground. This regression, combined with flat hallucination rates (42.9% Test vs. 41.8% Base in MENA, for example), indicates the model has not learned to distinguish between tasks requiring creative latitude and tasks requiring factual precision.\n\n---\n\n### 5.2 Training Roadmap: 1000-Sample Allocation\n\nThe fine-tuning budget should be allocated to address the specific failure modes identified through the quantitative and qualitative analysis, prioritizing interventions with the highest expected impact on currently underperforming markets.\n\n**Allocation Framework:**\n\n| Priority | Focus Area | Samples | Target Metric | Expected Impact |\n|----------|------------|---------|---------------|-----------------|\n| 1 | Arabic Dialect Differentiation | 300 | LOC Major Rate | +3-5pp net win rate in ar_AE, ar_SA |\n| 2 | Cyrillic Language Coherence | 250 | Spelling/Grammar Major | -15pp error rate in ru_RU, uk_UA |\n| 3 | Factual Grounding for Q&A | 200 | TF Major Rate | +3pp win rate in Closed Q&A |\n| 4 | Multi-Constraint Complexity | 150 | Hard Prompt Win Rate | -5pp error rate on complex tasks |\n| 5 | East Asian Cultural Context | 100 | LOC Minor Rate | +2pp net win rate in ko_KR, zh_CN |\n\n**Priority 1: Arabic Dialect Differentiation (300 samples)**\n\nThe MENA region exhibits a specific, diagnosable failure: the model defaults to Modern Standard Arabic regardless of dialect instructions. The Wrong Dialect/Language subcategory shows rates of 38.1% (ar_AE), 24.9% (ar_EG), and 41.0% (ar_SA) for the Test Model\u2014actually worse than the Base Model in ar_AE and ar_SA.\n\nSample composition should include:\n- 100 samples with explicit dialect markers requiring Gulf Arabic, Egyptian Arabic, or Levantine Arabic with negative constraints (\"Do not use formal Arabic\" or \"Respond as a local would speak\")\n- 100 samples requiring cultural context specific to UAE, Saudi Arabia, or Egypt that cannot be correctly addressed in MSA\n- 100 samples demonstrating graceful handling when dialect-specific vocabulary does not exist, teaching the model to acknowledge limitations rather than defaulting to MSA\n\nThe expected outcome is a reduction in Wrong Dialect/Language rates to below 25% across MENA markets, which should translate to approximately +3-5 percentage points in net win rate for ar_AE (currently +1.0pp) and ar_SA (currently +5.8pp).\n\n**Priority 2: Cyrillic Language Coherence (250 samples)**\n\nRussian and Ukrainian markets show error rates exceeding 90%, with qualitative evidence documenting \"nonsensical words\" and \"gibberish\" at rates suggesting fundamental tokenization or language modeling failures. The Major Spelling/Grammar Issue subcategory shows rates of 48.9% (ru_RU) and 69.3% (uk_UA) for the Test Model.\n\nThis allocation acknowledges that 250 samples cannot fix a pre-training deficit but can target the most common failure patterns:\n- 150 samples of grammatically correct, coherent responses across the task categories where the model currently produces incoherent output (Brainstorming, Creative Writing, Open Q&A)\n- 100 samples with explicit grammar correction feedback, teaching the model to recognize and avoid the specific nonsensical constructions documented in the qualitative analysis\n\nThe expected outcome is modest: a reduction in error rates from 90% to approximately 75-80%, sufficient to make the model usable but not competitive. Full remediation requires pre-training data augmentation.\n\n**Priority 3: Factual Grounding for Q&A (200 samples)**\n\nThe Test Model loses ground in Closed Q&A (-1.9pp) and shows no improvement in Extraction (-0.1pp), indicating that behavioral improvements in formatting and conciseness have not extended to factual precision. Hallucination rates remain essentially flat globally.\n\nSample composition should include:\n- 100 samples with verifiable facts requiring citation or explicit acknowledgment of uncertainty\n- 50 samples where the correct answer is \"I don't know\" or \"I cannot verify this information\"\n- 50 samples demonstrating the difference between creative tasks (where elaboration is appropriate) and factual tasks (where precision is required)\n\nThe expected outcome is a reversal of the Closed Q&A regression, targeting a +3 percentage point improvement in win rate for this category.\n\n**Priority 4: Multi-Constraint Complexity (150 samples)**\n\nHard Prompts show a 59.6% error rate for the Test Model, only a 4.6 percentage point improvement over the Base Model. The qualitative analysis reveals compound failures where the model must simultaneously satisfy factual accuracy, stylistic requirements, and length constraints.\n\nSample composition should include:\n- 100 samples combining factual accuracy requirements with specific formatting or length constraints\n- 50 samples demonstrating graceful degradation when constraints conflict, teaching the model to prioritize accuracy over style when trade-offs are necessary\n\n**Priority 5: East Asian Cultural Context (100 samples)**\n\nKorean and Chinese Simplified show net win rates of only +4.1pp and +4.0pp respectively, significantly below the global average of +11.6pp. The LOC No Issues rate for ko_KR is only 31.6%, suggesting cultural context gaps.\n\nSample composition should include:\n- 50 Korean samples with cultural context requiring local knowledge\n- 50 Chinese Simplified samples addressing the distinction between simplified and traditional character contexts\n\n---\n\n### 5.3 Prompt Engineering Guidelines\n\nThe Test Model's behavioral profile\u2014improved conciseness and formatting compliance with unchanged factual accuracy\u2014creates specific opportunities and risks for prompt engineering. The following guidelines are designed to maximize the model's strengths while mitigating its documented weaknesses.\n\n**Leverage the Conciseness Improvement:**\n\nThe Test Model shows a +13.6 percentage point shift toward neutral verbosity (50.5% \u2192 64.1%), with dramatic reductions in verbose output (-6.8pp at +2, -7.7pp at +1). This improvement is most pronounced in en_IN (+25.1pp neutral shift) and hi_Latn (+27.0pp neutral shift).\n\nFor tasks where conciseness is valued, prompts should explicitly request brevity:\n- \"Provide a concise summary in 3-4 sentences\"\n- \"List the key points without elaboration\"\n- \"Answer directly without preamble\"\n\nThe model will comply with these constraints more reliably than the Base Model, and the output quality in terms of formatting and structure will be higher.\n\n**Mitigate Factual Accuracy Risks:**\n\nFor queries requiring factual precision, prompts should include explicit grounding instructions:\n- \"Only include information you can verify. If uncertain, say so.\"\n- \"Do not invent names, dates, or statistics\"\n- \"If this requires specialized knowledge you may not have, acknowledge that limitation\"\n\nThese instructions leverage the model's improved instruction-following capability to partially compensate for its unchanged truthfulness baseline.\n\n**Handle Dialect Requirements Explicitly:**\n\nFor MENA markets, prompts must include redundant dialect specification:\n- Primary instruction: \"Respond in Egyptian Arabic (not Modern Standard Arabic)\"\n- Reinforcement: \"Use colloquial vocabulary and phrasing as spoken in Cairo\"\n- Negative constraint: \"Do not use formal or literary Arabic\"\n\nThis triple-specification approach addresses the model's documented tendency to default to MSA regardless of single dialect instructions.\n\n**Avoid Compound Complexity Without Prioritization:**\n\nThe model's 59.6% error rate on Hard Prompts indicates difficulty with multi-constraint tasks. When complexity is unavoidable, prompts should include explicit priority ordering:\n- \"Your primary goal is factual accuracy. Secondary goal is conciseness. If these conflict, prioritize accuracy.\"\n- \"First ensure the information is correct, then format according to the following guidelines...\"\n\n**Regional Prompt Adaptations:**\n\n| Region | Adaptation | Rationale |\n|--------|------------|-----------|\n| MENA | Triple dialect specification with negative constraints | 38-41% wrong dialect rates |\n| Eastern Europe | Avoid creative or open-ended tasks; prefer structured queries | 90% error rates on generative tasks |\n| East Asia | Include cultural context markers; specify formality level | Below-average localization scores |\n| Western Europe | Standard prompting; model performs well | +11pp average net win rate |\n| South Asia | Explicit length constraints; model tends toward appropriate verbosity | +25pp neutral verbosity shift in en_IN |\n\n**Task-Specific Guidelines:**\n\nFor **Brainstorming and Creative Writing** (where the model excels with +21pp and +16pp net win rates respectively), prompts can be relatively open-ended. The model's improved formatting and reduced verbosity will produce cleaner output without extensive constraint specification.\n\nFor **Closed Q&A and Extraction** (where the model shows regression or no improvement), prompts should include:\n- Explicit accuracy requirements\n- Permission to acknowledge uncertainty\n- Constraints against elaboration beyond the query scope\n\nFor **Summarization** (moderate improvement at +5.8pp), the model's conciseness improvements align well with task requirements. Prompts specifying target length will be followed more reliably than with the Base Model.\n\n---\n\n### 5.4 Monitoring and Iteration Framework\n\nPost-deployment monitoring should focus on the identified risk areas:\n\n**Key Performance Indicators:**\n\n| Metric | Threshold | Action if Breached |\n|--------|-----------|-------------------|\n| Closed Q&A Win Rate | <33% (current: 35.9%) | Escalate for additional factual grounding training |\n| MENA Dialect Compliance | <70% correct dialect | Trigger dialect-specific fine-tuning sprint |\n| ru_RU/uk_UA Error Rate | >85% | Maintain release hold; escalate for pre-training review |\n| Hallucination Rate (flagged) | >5% user-reported | Implement additional safety constraints |\n\n**Feedback Loop Structure:**\n\nWeek 1-2: Baseline establishment in released markets\nWeek 3-4: Initial trend analysis; identify any regression from evaluation performance\nMonth 2: First fine-tuning iteration using Priority 1-2 samples\nMonth 3: Reassess delayed release markets for potential promotion\n\nThe Test Model represents a genuine improvement in user experience through behavioral refinement. Its limitations in factual accuracy and regional coverage are addressable through targeted intervention, but deployment decisions must account for the gap between perceived quality (formatting, conciseness) and actual quality (truthfulness, cultural appropriateness). The tiered release strategy ensures that markets receive the model only when the improvement profile justifies the deployment risk."}], "variant": 1}